import numpy as np
import pandas as pd

"""
Parameter update has been under great scrutiny just because of the fact that
it can be more and better optimised.

Research is still going on and a new and better formula for Parameter updates is always welcome.

"""


def gradient_descent():
	"""
	TODO
		This is the simplest of them all and performs the worst.

	"""

	return grad

def momentum():
	"""
	TODO
		This is a simple upgradation of gradient descent and performs much much faster than
		the simple gradient descent. This one uses a velocity and momentum to improve the
		performance of the simple gradient descent

	"""

	return grad

def adagrad():
	"""
	TODO

	"""
